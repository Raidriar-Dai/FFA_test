import torch
import torch.nn as nn
from data_load import CIFAR_loaders
from data_overlay import overlay_y_on_x
from full_linear_net import CIFAR_Net

import wandb
import yaml

# 4 Experiments with CIFAR-10

def training_one_run():
    with open('./config.yaml') as file:
        config = yaml.load(file, Loader=yaml.FullLoader)

    wandb.init(project="FFA_test1", entity="raidriar_dai", config=config)

    # define hyper_parameters from wandb.config
    batch_size = wandb.config.batch_size
    dims = wandb.config.dims
    lr = wandb.config.lr
    threshold = wandb.config.threshold
    num_epochs = wandb.config.num_epochs
    weight_decay = wandb.config.weight_decay
    dropout = wandb.config.dropout

    # start sweeping with pre-defined hyper_parameters
    torch.manual_seed(1234)
    train_loader, test_loader = CIFAR_loaders(train_batch_size = batch_size)
    num_batches = len(train_loader)

    net = CIFAR_Net(dims, lr, threshold, num_epochs, weight_decay, dropout)

    # Set the net as training mode (actually it's by default in training mode):
    net.train()
    average_train_error = []
    for i, batch in enumerate(train_loader):
        print(f"Current Training batch: {i+1} / {num_batches}")
        x, y = batch
        x, y = x.cuda(), y.cuda()
        x_pos = overlay_y_on_x(x, y)
        rnd = torch.randperm(x.size(0))
        x_neg = overlay_y_on_x(x, y[rnd])

        net.forward_train(x_pos, x_neg)

        batch_error = 1.0 - net.predict(x).eq(y).float().mean().item()
        average_train_error.append(batch_error)
        # WANDB: logging training error of each batch
        wandb.log({
            "batch": i+1, 
            "batch_error": batch_error
            })
    average_train_error = sum(average_train_error) / len(average_train_error)
    wandb.log({"average_train_error": average_train_error}, commit=False)
    print("average_train_error:", average_train_error)

    # Set the net as testing mode:
    net.eval()
    # A new train loader with batch_size = 10000, generated by a different seed.
    torch.manual_seed(4321)
    train_loader_for_evaluation, _ = CIFAR_loaders(train_batch_size=10000)
    x, y = next(iter(train_loader_for_evaluation))
    x, y = x.cuda(), y.cuda()
    final_train_error = 1.0 - net.predict(x).eq(y).float().mean().item()
    wandb.log({"final_train_error": final_train_error}, commit=False)
    print("final_train_error:", final_train_error)

    # Set the net as testing mode:
    net.eval()
    x_te, y_te = next(iter(test_loader))
    x_te, y_te = x_te.cuda(), y_te.cuda()
    test_error = 1.0 - net.predict(x_te).eq(y_te).float().mean().item()
    wandb.log({"test_error": test_error})
    print("test_error:", test_error)

    wandb.finish()

training_one_run()