import torch
import torch.nn as nn
from data_load import CIFAR_loaders
from data_overlay import overlay_y_on_x
from full_linear_net import CIFAR_Net

import wandb
import yaml

# 4 Experiments with CIFAR-10

def training_one_run():
    with open('/home/intern/scratch/qirundai/FFA13/FFA_test/config_CIFAR.yaml') as file:
        config = yaml.load(file, Loader=yaml.FullLoader)
    wandb.init(project="FFA_test1", entity="raidriar_dai", config=config)

    # define hyper_parameters from wandb.config
    batch_size = wandb.config.batch_size
    dims = wandb.config.dims
    lr = wandb.config.lr
    threshold = wandb.config.threshold
    num_epochs = wandb.config.num_epochs
    weight_decay = wandb.config.weight_decay
    dropout = wandb.config.dropout

    torch.manual_seed(1234)
    global_epochs = 10  # Number of epochs in global training process
    net = CIFAR_Net(dims, lr, threshold, num_epochs, weight_decay, dropout)
    train_loader, test_loader = CIFAR_loaders(train_batch_size = batch_size)
    num_batches = len(train_loader)
    
    # start sweeping with pre-defined hyper_parameters
    for epoch in range(global_epochs):
        
        # Set the net as training mode (actually it's by default in training mode):
        net.train()
        average_train_error = []
        for i, batch in enumerate(train_loader):
            print(f"Current Global Epoch: {epoch+1} / {global_epochs}; Current Training Batch: {i+1} / {num_batches}")
            x, y = batch
            x, y = x.cuda(), y.cuda()
            x_pos = overlay_y_on_x(x, y)
            rnd = torch.randperm(x.size(0))
            x_neg = overlay_y_on_x(x, y[rnd])

            net.forward_train(x_pos, x_neg)

            batch_error = 1.0 - net.predict(x).eq(y).float().mean().item()
            average_train_error.append(batch_error)
        average_train_error = sum(average_train_error) / len(average_train_error)

        # # Set the net as testing mode:
        # net.eval()
        # # A new train loader with batch_size = 10000, generated by a different seed.
        # torch.manual_seed(4321)
        # train_loader_for_evaluation, _ = CIFAR_loaders(train_batch_size=10000)
        # x, y = next(iter(train_loader_for_evaluation))
        # x, y = x.cuda(), y.cuda()
        # final_train_error = 1.0 - net.predict(x).eq(y).float().mean().item()
        # wandb.log({"final_train_error": final_train_error}, commit=False)
        # print("final_train_error:", final_train_error)

        # Set the net as testing mode:
        net.eval()
        x_te, y_te = next(iter(test_loader))
        x_te, y_te = x_te.cuda(), y_te.cuda()
        test_error = 1.0 - net.predict(x_te).eq(y_te).float().mean().item()
        
        wandb.log({
            "global epoch": epoch+1,
            "final batch error": batch_error,
            "average train error": average_train_error,
            "test error": test_error
            })
        
        print(f"global epoch: {epoch+1}, average train error: {average_train_error:.3f}, "
              f"final batch error: {batch_error:.3f}, test error: {test_error:.3f}")
    
    wandb.finish()

training_one_run()